<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta name="description" content="Shuai Ma is a PhD student at HKUST">
    <link rel="stylesheet" href="./personal_webpage_files/bootstrap.min.css">
    <link rel="stylesheet" href="./personal_webpage_files/all.min.css">
    <base href=".">
    <title>Shuai Ma (马帅)</title>
    <style>
        body {
            margin-top: 20px;
            margin-bottom: 30px;
            font-family: sans-serif;
            font-weight: lighter;
        }

        a {
            color: black;
            border-bottom: 1px dotted black;
        }

        a:hover,
        a:active {
            color: #2774AE;
            text-decoration: none;
        }

        h1 a {
            color: #6B747C;
            border: none;
        }

        h2 {
            font-size: 1.5em;
            border-bottom: 2px solid;
        }

        h3 {
            font-size: 1.2em;
            color: #000000;
        }

        h4 {
            font-size: 1em;
            font-family: sans-serif;
            font-weight: lighter;
            color: #000000;
            margin-top: 10px;
            margin-bottom: 30px;
        }

        .strong {
            color: #2774AE;
        }

        @media (max-width: 767.98px) {
            header {
                text-align: center;
            }
        }

        ul.social-icons {
            font-size: 1rem;
            margin-top: 24px;
            margin-bottom: 0;
        }

        ul.social-icons li::before {
            content: '[';
        }

        ul.social-icons li::after {
            content: ']';
        }

        ul.social-icons li a {
            border: none;
        }

        img.portrait {
            max-width: 100%;
        }

        @media (max-width: 767.98px) {
            img.portrait {
                display: block;
                max-width: 300px;
                margin: auto;
            }
        }

        .annotation {
            margin-top: -0.5em;
            margin-bottom: 0.5em;
            font-size: 12px;
            line-height: 12px;
        }

        .taxonomy img {
            max-width: 100%;
        }

        div.research-project {
            font-size: 14px;
            margin-bottom: 1.5rem;
        }

        div.line-of-research {
            background-color: #F0F0F0;
        }

        div.research-project video {
            max-width: 100%;
            margin-bottom: 0.5rem;
        }

        div.research-project p {
            margin-bottom: 0.3rem;
        }

        .news {
            font-size: 15px;
            margin-bottom: 0px;
        }

        #news-more {
            display: none;
        }

        .tweets {
            overflow: auto;
            -webkit-overflow-scrolling: touch;
        }

        .info {
            border-style: none;
            font-weight: bold;
            color: #999;
        }

        hr.dash {
            border-top: 1px dashed #bbbbbb;
            margin-bottom: 15px;
            margin-top: 15px;
        }

        .switch {
            position: relative;
            display: block;
            width: 32px;
            height: 18px;
            float: left;
			top: 3px;
        }

        .slider {
            position: absolute;
            cursor: pointer;
            top: 3px;
            right: 0;
            bottom: -3px;
            left: 0;
            background-color: #ccc;
            transition: 0.5s;
        }

        .slider:before {
            position: absolute;
            content: "";
            height: 12px;
            width: 12px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: 0.5s;
        }

        .active .slider {
            background-color: #DB522F;
        }

        .active .slider:before {
            transform: translateX(14px);
        }
		
		.slider.round {
		  border-radius: 34px;
		}

		.slider.round:before {
		  border-radius: 50%;
		}
    </style>
	

<!-- <script charset="utf-8" src="./Yang Zhang _ Sensing research at UCLA_files/moment_timeline.4391e0bf4053fbaa2a022e3fad2a1e1a.js.下载"></script><script charset="utf-8" src="./Yang Zhang _ Sensing research at UCLA_files/timeline.34cf38a85ac899f1d6a0438a1659decc.js.下载"></script></head> -->

<body data-new-gr-c-s-check-loaded="14.1050.0" data-gr-ext-installed="">
	
	<!-- Google Tag Manager (noscript) -->
	<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PFGW5C3"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<!-- End Google Tag Manager (noscript) -->
	
    <header class="container">
        <h1 class="float-md-left mb-0">
            <a href="https://shuaima.cc/">Shuai Ma (马帅)</a>
        </h1>
        <ul class="list-inline float-md-right social-icons">
            <li class="list-inline-item"><a href="./personal_webpage_files/cv.pdf">CV/Resume</a></li>
            <li class="list-inline-item"><a href="https://scholar.google.com/citations?hl=en&user=qajd8BYAAAAJ">Google
                    Scholar</a></li>
            <li class="list-inline-item"><a href="https://github.com/mashuaiwudi">Github</a></li>
            <li class="list-inline-item"><a href="mailto:shuai.ma@connect.ust.hk">Email</a></li>
            <!-- <li class="list-inline-item"><a href="https://hilab.dev/">HiLab</a></li> -->
        </ul>
        <div class="clearfix"></div>
        <hr>
    </header>

    <div class="container">
        <div class="row mb-3">
            <div class="col-xl-7 col-lg-8 col-md-8">
				
	            <!-- <p> <b><i><span style="color: #FF0000;">Update:</span> I am hosting internships for students at UCLA who have research interests in HCI. Please drop me an email if you are interested in working with me on building future human-computer interactive systems! </i></b> 
				 </p> -->
			
                <p>
					Hi, this is Shuai. I am a 2nd year PhD student in HCI Lab at Hong Kong University of Science and Technology (HKUST) supervised by <a href="https://www.cse.ust.hk/~mxj/">Prof. Xiaojuan Ma</a>. I got my master degree from HCI lab of Chinese Academy of Science, supervised by Prof. Feng Tian, <a href="https://lcs.ios.ac.cn/~xiangmin/">Prof. Xiangmin Fan</a>, and <a href="http://www.jin-huang.net/homepage/">Prof. Jin Huang</a>.
                    <br><br>My research interest lies in Human-AI Interaction. Previously, I utilized advanced CV and NLP techniques to build interactive tools for users' daily activities (e.g., photo-taking, video-editing), Education (e.g., MOOC learning, online classes), and Healthcare (e.g., Parkinson’s disease detection, online health community support). Currently, I am focusing on fostering transparent communication in human-human interaction and human-AI interaction with human-centered approaches. Also, I am interested in human-centered explainable AI (HCXAI).

                </p>
				
                <!-- <p>
					I publish at ACM <a href="https://dl.acm.org/conference/chi">CHI</a>, <a href="https://dl.acm.org/conference/uist">UIST</a>, and <a href="https://dl.acm.org/journal/imwut"> IMWUT </a> and have received 2 Best Paper and 4 Honorable Mention Awards. Taxonomies of my completed research can be found below.
                </p> -->
           
			 
            </div>
            <div class="offset-xl-2 col-lg-3 col-md-3">
                <img src="./personal_webpage_files/portrait.jpg" class="portrait" alt="a portrait of shuai ma">
            </div>
        </div>

        <!-- <div class="row taxonomy">
			
            <div class="col-lg-6 mb-6">
                <img src="./Yang Zhang _ Sensing research at UCLA_files/Taxonomy.png" alt="a diagram of my research focus, which includes wearable sensing, mixed reality, and sensate environments">
            </div>
			
			<div style="margin-top:20px;" class="col-lg-8 mb-8"><p class="annotation">[Research focus diagram inspired by professor <a href="https://people.eecs.berkeley.edu/~bjoern/">Bjoern Hartmann</a>]</p></div>
        </div> -->

        <div class="row">
            <!-- left column -->
            <div class="col-lg-8 mb-2">
                <h2>
                    Research
					<!--
                    <div class="float-right">
                        <small class="ml-2">by category</small>
                        <div class="switch sort-by-date">
                            <span class="slider round"></span>
                        </div>
                    </div>
					-->
                </h2>
				 
                <div class="research-projects">
					
                <!-- <div class="alert alert-secondary">
                    <h3>Please refer to <a href="https://www.hilab.dev/">HiLab's webpage</a> for more recent work (after 01/01/2021)</h3>
                </div> -->
					
					
                <div class="row research-project" data-sort="2022">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/glancee.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							Glancee: An Adaptable System for Instructors to Grasp Student Learning Status in Synchronous Online Classes
                        </h6>
                        <p class="text-muted">
                            Shuai Ma, Taichang Zhou, Fei Nie, Xiaojuan Ma. (CHI 2022)<br>

							<!-- <a class="info" href="https://vimeo.com/394833754">[Video]</a> -->
                            <!-- <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3432232">[DOI]</a> -->
                            <a class="info">[PDF(to release)]</a>
                        </p>
                       
                        <p>
							Focusing on synchronous online classes (e.g., real-time Zoom-based classes), Glancee address instructors’ difficulty to observe students’ learning status due to students’ unwillingness to show their videos. Specifically, we mitigate the gap that lack of empirical investigation on instructors’ preferences and lack of exploration of designing adaptable systems to meet the needs of individual instructors. 
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>

                <div class="row research-project" data-sort="2019">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/smarteye.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							SmartEye: Assisting Instant Photo Taking viaIntegrating User Preference with Deep View Proposal Network
                        </h6>
                        <p class="text-muted">
                            Shuai Ma, Zijun Wei, Feng Tian, Xiangmin Fan, Jianming Zhang, Xiaohui Shen, Zhe Lin, Jin Huang, Radomir Mech, Dimitris Samaras, Hongan Wang. (CHI 2019)<br>

							<!-- <a class="info" href="https://vimeo.com/394833754">[Video]</a> -->
                            <!-- <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3432232">[DOI]</a> -->
                            <a class="info", href="./personal_webpage_files/paper_pdf/SmartEye.pdf">[PDF]</a>
                        </p>
                       
                        <p>
							We design SmartEye which incorporates
                            users' individual preference of photo composition into a Deep Learning-based AI model.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>


                <div class="row research-project" data-sort="2021">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/cass.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							CASS: Towards Building a Social-Support Chatbot for Online Health Community
                        </h6>
                        <p class="text-muted">
                            Liuping Wang, Dakuo Wang, Feng Tian, Zhenhui Peng, Xiangmin Fan, Zhan Zhang, Shuai Ma, Mo Yu, Xiaojuan Ma, Hongan Wang. (CSCW 2021)<br>

							<!-- <a class="info" href="https://vimeo.com/394833754">[Video]</a> -->
                            <!-- <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3432232">[DOI]</a> -->
                            <a class="info", href="./personal_webpage_files/paper_pdf/cass.pdf">[PDF]</a>
                        </p>
                       
                        <!-- <p>
							CASS provides social support for community members in an online health community.
                        </p> -->
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>

                

                <div class="row research-project" data-sort="2021">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/reminder.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							What Did I Miss? Assisting User adaptive Missed Content Reviewing in MOOC Learning
                        </h6>
                        <p class="text-muted">
                            Qian Zhu, Shuai Ma. (UIST 2019 Adjunct)<br>

							<!-- <a class="info" href="https://vimeo.com/394833754">[Video]</a> -->
                            <!-- <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3432232">[DOI]</a> -->
                            <a class="info", href="./personal_webpage_files/paper_pdf/reminder.pdf">[PDF]</a>
                        </p>
                       
                        <!-- <p>
							We present <a>Reminder</a>, a system for detecting divided attention and reminding learners what they just missed on both PC and mobile devices with a camera capturing their status.
                        </p> -->
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>

                <div class="row research-project" data-sort="2021">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/prescreen.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							Pre-screen: Assisting Material Screening in Early-stage of Video Editing
                        </h6>
                        <p class="text-muted">
                            Qian Zhu, Shuai Ma, Cuixia Ma. (UIST 2019 Adjunct)<br>

							<!-- <a class="info" href="https://vimeo.com/394833754">[Video]</a> -->
                            <!-- <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3432232">[DOI]</a> -->
                            <a class="info", href="./personal_webpage_files/paper_pdf/prescreen.pdf">[PDF]</a>
                        </p>
                       
                        <!-- <p>
							We present <a>Reminder</a>, a system for detecting divided attention and reminding learners what they just missed on both PC and mobile devices with a camera capturing their status.
                        </p> -->
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>

                <div class="row research-project" data-sort="2021">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/smartphonepd.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							Implicit Detection of Motor Impairment in Parkinson’s Disease from Everyday Smartphone Interactions
                        </h6>
                        <p class="text-muted">
                            Jing Gao, Feng Tian, Junjun Fan, Dakuo Wang, Xiangmin Fan, Yicheng Zhu, Shuai Ma, Jin Huang, Hongan Wang. (CHI EA 2018)<br>

							<!-- <a class="info" href="https://vimeo.com/394833754">[Video]</a> -->
                            <!-- <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3432232">[DOI]</a> -->
                            <a class="info", href="./personal_webpage_files/paper_pdf/smartphonepd.pdf">[PDF]</a>
                        </p>
                       
                        <!-- <p>
							We present <a>Reminder</a>, a system for detecting divided attention and reminding learners what they just missed on both PC and mobile devices with a camera capturing their status.
                        </p> -->
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>

                <div class="row research-project" data-sort="2021">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/mirroru.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							mirrorU: Scaffolding Emotional Reflection via In-Situ Assessment and Interactive Feedback
                        </h6>
                        <p class="text-muted">
                            Liuping Wang, Xiangmin Fan, Feng Tian, Lingjia Deng, Shuai Ma, Jin Huang, Hongan Wang. (CHI EA 2018)<br>

							<!-- <a class="info" href="https://vimeo.com/394833754">[Video]</a> -->
                            <!-- <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3432232">[DOI]</a> -->
                            <a class="info", href="./personal_webpage_files/paper_pdf/mirroru.pdf">[PDF]</a>
                        </p>
                       
                        <!-- <p>
							We present <a>Reminder</a>, a system for detecting divided attention and reminding learners what they just missed on both PC and mobile devices with a camera capturing their status.
                        </p> -->
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>


					
					
					

            </div>
            </div>
            <!-- /left column -->
            <!-- right column -->
            <div class="col-lg-4 mb-2">
				
                <h2>Latest News</h2>
                <ul class="news" style="font-size: 13px">
                    <li>2022-4-19 | I passed my PhD Qualifying Exam and became a PhD candidate! Thanks for my committee members' valuable feedback!</li>
                    <li>2022-3-10 | Happy to be a student volunteer at CHI 2022!</li>
					<li>2022-2-10 | Our paper <i>Glancee</i> is conditionally accepted at CHI 2022.</li>
                                </ul>
				<br>
				<br>
				
				
                <h2>Teaching Assistant Service</h2>
                <ul class="news" style="font-size: 13px">
					<li>2021-2022 Fall COMP 1021 - Introduction to Computer Science</li>
					<li>2020-2021 Spring COMP 1021 - Introduction to Computer Science</li>
                </ul>
				<br>
				
                <h2>Awards</h2>
                <ul class="news" style="font-size: 13px">
                    <li>2020 Huawei PhD Fellowship</li>
                    <li>2020 HKUST Redbird PhD Scholarship</li>
					<li>2019 CHI Honorable Mention Award</li>
					<li>2019 President Scholarship in Chinese Academy of Sciences (1%)</li>
					<li>2018 National Scholarship for Graduate (1%)</li>
					<li>2018 Pacemaker of Merit Student of UCAS (1%)</li>
                    <li>2018 <b>Winner</b> of Huawei Cup Free Software Programming Competition</li>
					<li>2017 Excellence Award for Science Creation Program of Chinese Academy of Sciences</li>
					<li>2017 Special Scholarship for Undergraduates & the best ten students of HIT (0.1%)</li>
					<li>2017 Excellent Graduates in Harbin Institute of Technology</li>
					<li>2016 National Scholarship for Undergraduate (1%)</li>
                    <li>2016 Pacemaker of Merit Student of HIT (0.1%)</li>
                    <li>2016 Pacemaker to Merit Student Cadres of HIT (0.1%)</li>
                    <li>2016 <b>Winner</b> of The 5th National Marine Vehicle Design and Manufacturing Competition</li>
                    <li>2016 <b>Winner</b> of The 18th National Robot Championship</li>
					
                </ul>
				<br>
				
                <h2>Service</h2>
				
				<p style="font-size: 13px">
					Conference Review: CHI'19, '20, '22, CHI EA'22, WWW'21 <br>
					Journal Review: CCF Transactions on Pervasive Computing and Interaction
				</p>
				
<!-- 	
				<br>
                <h2>Press</h2>
                <ul class="news" style="font-size: 13px">
					<li>TechCrunch: <a href="https://techcrunch.com/2018/10/15/new-tech-lets-robots-feel-their-environment/">This robot uses lasers to ‘listen’ to its environment</a></li>
					<li>Hackaday: <a href="https://hackaday.com/2018/10/22/vibrosight-hears-when-you-are-sleeping-it-knows-when-youre-awake/">Vibrosight hears when you are sleeping. It knows when you're awake</a></li>
					<li>Fast Company: <a href="https://www.fastcompany.com/90168954/turn-your-wall-into-a-touchscreen-for-20">Turn Your Wall Into A Touch Screen For $20</a></li>
					<li>NBC News: <a href="https://www.nbcnews.com/mach/science/new-smart-wall-lets-you-control-your-home-swipes-taps-ncna869006">New smart wall lets you control your home with swipes, taps</a></li>
					<li>Engadget: <a href="https://www.engadget.com/2018/04/24/touch-sensitive-wall-control-home-devices/">Touch-sensitive wall might let you control home devices in the future</a></li>
					<li>Digital Trends: <a href="https://www.digitaltrends.com/cool-tech/carnegie-mellon-smart-walls/">This conductive paint transforms regular walls into giant touchpads</a></li>
					<li>The Verge: <a href="https://www.theverge.com/circuitbreaker/2018/4/28/17289976/smart-wall-carnegie-mellon-disney-home">You may soon be able to control your home with a smart wall</a></li>
					<li>Architect Magazine: <a href="https://www.architectmagazine.com/technology/transforming-walls-into-smart-surfaces_o">Transforming Walls into Smart Surfaces</a></li>
					<li>Science Magazine: <a href="https://www.sciencemag.org/news/2018/04/watch-researchers-turn-wall-alexa-s-eyes-and-ears">Watch researchers turn a wall into Alexa’s eyes and ears</a></li>
					<li>MIT Technology Review: <a href="https://www.technologyreview.com/s/604337/a-cheap-simple-way-to-make-anything-a-touch-pad/">A Cheap, Simple Way to Make Anything a Touch Pad</a></li>
					<li>New Scientist: <a href="https://www.newscientist.com/article/2130534-spray-on-touch-controls-give-an-interactive-twist-to-any-surface/">Spray-on touch controls give an interactive twist to any surface</a></li>
					<li>The Wall Street Journal: <a href="https://www.wsj.com/articles/how-to-turn-anything-into-a-touchpad-1494604386">How to Turn Anything into a Touchpad</a></li>
					<li>The Verge: <a href="https://www.theverge.com/2017/5/8/15577390/electrick-spray-on-touch-controls-future-interfaces-group">Electrick lets you spray touch controls onto any object or surface</a></li>
					<li>Engadget: <a href="https://www.engadget.com/2017/05/08/electrick-paint-touch-input/">Get ready to 'spray' touch controls onto any surface</a></li>
					<li>CNET: <a href="https://www.cnet.com/news/electrick-touchpad-spray-paint-carnegie-mellon/">Almost anything can become a touchpad with some spray paint</a></li>
					<li>Popular Science: <a href="https://www.popsci.com/why-touch-sensitive-brain-made-out-jell-o-represents-smart-idea/">What a Jell-O brain tells us about the future of human-machine interaction</a></li>
					<li>Gizmodo: <a href="https://gizmodo.com/scientists-figure-out-how-to-turn-anything-into-a-touch-1795016303">Scientists Figure Out How to Turn Anything Into a Touchscreen Using Conductive Spray Paint</a></li>
					<li>TechCrunch: <a href="https://techcrunch.com/2017/05/08/new-technique-turns-anything-into-a-touch-sensor/">New technique turns anything into a touch sensor</a></li>
					<li>Pittsburgh Post-Gazette <a href="https://www.post-gazette.com/business/tech-news/2017/06/14/Touch-sensing-technology-Electrick-pittsburgh-future-interfaces-group-cmu/stories/201706070162">Touch-sensing technology born of CMU researchers grabs companies' interest</a></li>
					<li>TechCrunch: <a href="https://techcrunch.com/2017/05/11/google-funded-super-sensor-project-brings-iot-powers-to-dumb-appliances/">Google-funded ‘super sensor’ project brings IoT powers to dumb appliances</a></li>
					<li>MIT Technology Review: <a href="https://www.technologyreview.com/s/601405/use-your-arm-as-a-smart-watch-touch-pad/">Use Your Arm as a Smart-Watch Touch Pad</a></li>
					<li>The Verge: <a href="https://www.theverge.com/circuitbreaker/2018/4/27/17289572/lumiwatch-projector-smartwatch-arm-touch-screen">New tech turns your skin into a touchscreen for your smartwatch</a></li>
					<li>Engadget: <a href="https://www.engadget.com/2016/05/05/touchscreen-skin-smartwatch-tech/">Navigate your smartwatch by touching your skin</a></li>
					<li>Gizmodo: <a href="https://gizmodo.com/this-new-skinterface-could-make-smartwatches-suck-less-1774926857">This New 'Skinterface' Could Make Smartwatches Suck Less</a></li>
					<li>CNET: <a href="https://www.cnet.com/news/skintrack-turns-your-entire-forearm-into-a-smartwatch-touchpad/">SkinTrack turns your entire forearm into a smartwatch touchpad</a></li>
					<li>WIRED: <a href="https://www.wired.com/2016/05/device-turns-arm-touchpad-heres-works/">SkinTrack Turns Your Arm Into a Touchpad</a></li>
					<li>Gizmodo: <a href="https://gizmodo.com/this-smartwatch-detects-gestures-by-watching-the-muscle-1741490619">This Smartwatch Detects Gestures By Watching the Muscles Inside Your Arm Move</a></li>
					<li>Hackaday: <a href="https://hackaday.com/2015/11/12/impedance-tomography-is-the-new-x-ray-machine/">Impedance Tomography is the new X-ray Machine</a></li>
					<li>New Scientist: <a href="https://www.newscientist.com/article/dn28475-no-touch-smartwatch-scans-the-skin-to-see-the-world-around-you/">No-touch smartwatch scans the skin to see the world around you</a></li>
					
                </ul>
				
                <div class="mt-3 tweets">
                    <iframe id="twitter-widget-0" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" class="twitter-timeline twitter-timeline-rendered" style="position: static; visibility: visible; display: inline-block; width: 100%; height: 2400px; padding: 0px; border: none; max-width: 100%; min-width: 180px; margin-top: 0px; margin-bottom: 0px; min-height: 200px;" data-widget-id="profile:yanghci" title="Twitter Timeline" src="./Yang Zhang _ Sensing research at UCLA_files/saved_resource.html"></iframe>
                    <script async="" src="./Yang Zhang _ Sensing research at UCLA_files/widgets.js.下载" charset="utf-8"></script>
                </div> -->
				
            </div>
            <!-- /right column -->
        </div>

    </div>

<footer>
    <div class="container">
		<small> © 2022 - 2022 All rights reserved. Webpage template from <a href="https://www.yangzhang.dev/">Yang Zhang</a></small>
        <!-- <div style="margin-top:20px;" class="col-lg-8 mb-8"><p class="annotation">[Research focus diagram inspired by professor <a href="https://people.eecs.berkeley.edu/~bjoern/">Bjoern Hartmann</a>]</p></div> -->
	</div>
        
</footer>





<script src="./Yang Zhang _ Sensing research at UCLA_files/jquery.min.js.下载"></script>
<script>
    $('#toggle-more-news').click(function () {
        $('#news-more').slideToggle();
        $('#news-more').is(':visible') ? $(this).text('< Hide') : $(this).text('More >');
        return false;
    });

    $(window).on('scroll', function () {
        $('video').each(function () {
            var video = this;
            var rect = video.getBoundingClientRect();

            if (
                rect.>= 0 && rect.left >= 0 &&
                rect.bottom <= $(window).height() &&
                rect.right <= $(window).width()
            ) {
                video.play();
            } else {
                video.pause();
            }
        });
    });

    var researchProjects = $('.research-projects').html();

    var researchProjectsSorted = $('div.research-project').sort(function (a, b) {
        var contentA = $(a).attr('data-sort');
        var contentB = $(b).attr('data-sort');
        return (contentA < contentB) ? 1 : (contentA > contentB) ? -1 : 0;
    });

    $('.sort-by-date').click(function () {
        $(this).toggleClass('active');
        if ($(this).hasClass('active')) {
            $('.research-projects').html(researchProjects);
        } else {
            $('.research-projects').html(researchProjectsSorted);
        }
    });
	
	

    document.querySelectorAll('.email-anchor').forEach(function(a) {
        a.href = 'mailto:' + ['shuai.ma', 'connect.ust.hk'].join('@');
    });
</script>


<!-- <iframe scrolling="no" frameborder="0" allowtransparency="true" src="./Yang Zhang _ Sensing research at UCLA_files/widget_iframe.a58e82e150afc25eb5372dd55a98b778.html" title="Twitter settings iframe" style="display: none;"></iframe><iframe id="rufous-sandbox" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" style="position: absolute; visibility: hidden; display: none; width: 0px; height: 0px; padding: 0px; border: none;" title="Twitter analytics iframe" src="./Yang Zhang _ Sensing research at UCLA_files/saved_resource(1).html"></iframe></body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html> -->