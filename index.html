<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta name="description" content="Shuai Ma is a PhD student at HKUST">
    <link rel="stylesheet" href="./personal_webpage_files/bootstrap.min.css">
    <link rel="stylesheet" href="./personal_webpage_files/all.min.css">
    <base href=".">
    <title>Shuai Ma</title>
    <style>
        body {
            margin-top: 20px;
            margin-bottom: 30px;
            font-family: sans-serif;
            font-weight: lighter;
        }

        a {
            color: black;
            border-bottom: 1px dotted black;
        }

        a:hover,
        a:active {
            color: #2774AE;
            text-decoration: none;
        }

        h1 a {
            color: #6B747C;
            border: none;
        }

        h2 {
            font-size: 1.5em;
            border-bottom: 2px solid;
        }

        h3 {
            font-size: 1.2em;
            color: #000000;
        }

        h4 {
            font-size: 1em;
            font-family: sans-serif;
            font-weight: lighter;
            color: #000000;
            margin-top: 10px;
            margin-bottom: 30px;
        }

        .strong {
            color: #2774AE;
        }

        @media (max-width: 767.98px) {
            header {
                text-align: center;
            }
        }

        .social-icons {
            font-size: 1.5rem;
            margin-top: 10px;
        }

        .social-icons li {
            display: inline;
            margin-right: 15px;
        }

        .social-icons li a {
            color: #000;
        }

        .social-icons li a:hover {
            color: #2774AE;
        }

        img.portrait {
            max-width: 100%;
        }

        @media (max-width: 767.98px) {
            img.portrait {
                display: block;
                max-width: 500px;
                margin: auto;
            }
        }

        .annotation {
            margin-top: -0.5em;
            margin-bottom: 0.5em;
            font-size: 12px;
            line-height: 12px;
        }

        .taxonomy img {
            max-width: 100%;
        }

        div.research-project {
            font-size: 14px;
            margin-bottom: 1.5rem;
        }

        div.line-of-research {
            background-color: #F0F0F0;
        }

        div.research-project video {
            max-width: 100%;
            margin-bottom: 0.5rem;
        }

        div.research-project p {
            margin-bottom: 0.3rem;
        }

        .news {
            font-size: 15px;
            margin-bottom: 0px;
        }

        #news-more {
            display: none;
        }

        .award {
            font-size: 15px;
            margin-bottom: 0px;
        }

        #award-more {
            display: none;
        }

        .tweets {
            overflow: auto;
            -webkit-overflow-scrolling: touch;
        }

        .info {
            border-style: none;
            font-weight: bold;
            color: #999;
        }

        hr.dash {
            border-top: 1px dashed #bbbbbb;
            margin-bottom: 15px;
            margin-top: 15px;
        }

        .switch {
            position: relative;
            display: block;
            width: 32px;
            height: 18px;
            float: left;
			top: 3px;
        }

        .slider {
            position: absolute;
            cursor: pointer;
            top: 3px;
            right: 0;
            bottom: -3px;
            left: 0;
            background-color: #ccc;
            transition: 0.5s;
        }

        .slider:before {
            position: absolute;
            content: "";
            height: 12px;
            width: 12px;
            left: 3px;
            bottom: 3px;
            background-color: white;
            transition: 0.5s;
        }

        .active .slider {
            background-color: #DB522F;
        }

        .active .slider:before {
            transform: translateX(14px);
        }
		
		.slider.round {
		  border-radius: 34px;
		}

		.slider.round:before {
		  border-radius: 50%;
		}

        .filter-buttons {
            text-align: center;
            margin-bottom: 20px;
        }

        .filter-buttons button {
            background-color: #b0b0b0; /* A neutral gray color */
            color: white;
            border: none;
            padding: 10px 15px;
            margin: 5px;
            border-radius: 5px;
            cursor: pointer;
            transition: background-color 0.3s, box-shadow 0.3s;
        }

        .filter-buttons button:hover {
            background-color: #8c8c8c; /* A darker shade of gray */
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        }



    </style>
	
<body data-new-gr-c-s-check-loaded="14.1050.0" data-gr-ext-installed="">
	
	<!-- Google Tag Manager (noscript) -->
	<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PFGW5C3"
	height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<!-- End Google Tag Manager (noscript) -->
	
    <header class="container">
        <h1 class="float-md-left mb-0">
            <a href="https://shuaima.cc/">Shuai Ma</a>
        </h1>
        <ul class="list-inline float-md-right social-icons">
            <li class="list-inline-item"><a href="https://scholar.google.com/citations?hl=en&user=qajd8BYAAAAJ"><i class="fab fa-google"></i></a></li>
            <!-- <li class="list-inline-item"><a href="https://github.com/mashuaiwudi"><i class="fab fa-github"></i></a></li> -->
            <li class="list-inline-item"><a href="mailto:shuai.ma@connect.ust.hk"><i class="fas fa-envelope"></i></a></li>
            <li class="list-inline-item"><a href="https://twitter.com/shuaima_hci"><i class="fab fa-twitter"></i></a></li>
        </ul>
        <div class="clearfix"></div>
        <hr>
    </header>




      
    <div class="container">
        <div class="row mb-3">
            <div class="col-xl-8 col-lg-8 col-md-8">
				
	            <!-- <p> <b><i><span style="color: #FF0000;">Update:</span> I am hosting internships for students at UCLA who have research interests in HCI. Please drop me an email if you are interested in working with me on building future human-computer interactive systems! </i></b> 
				 </p> -->
			
                <p>
                    Hey, this is Shuai. I am currently a Postdoctoral Researcher at the <a href="https://cbl.aalto.fi/">Computational Behavior Lab</a>, Aalto University, where I work with <a href="https://users.aalto.fi/~oulasvir/">Prof. Antti Oulasvirta</a>. 
                    Previously, I received my PhD from the HCI Lab at the <a href="https://hkust.edu.hk/">Hong Kong University of Science and Technology (HKUST)</a>, where I had the privilege of being advised by <a href="https://www.cse.ust.hk/~mxj/">Prof. Xiaojuan Ma</a>.
                    <!-- Previously, I was trained in HCI (my master's degree) from the HCI lab of Chinese Academy of Science, supervised by <a href="https://people.ucas.ac.cn/~fengt">Prof. Feng Tian</a>. I also luckily worked with <a href="https://lcs.ios.ac.cn/~xiangmin/">Prof. Xiangmin Fan (Chinese Academy of Science, China)</a>, <a href="https://www.dakuowang.com/">Prof. Dakuo Wang (Northeastern University, USA)</a>, <a href="https://mingyin.org/">Prof. Ming Yin (Purdue University, USA)</a>. -->
                    <br><br>
                    My research lies in human-computer interaction (HCI), with a particular emphasis on developing human-AI collaboration systems that advance hybrid intelligence and enhance complementary human-AI team performance (1 + 1 > 2).
                    <br>
                    <ul>
                        
                        <li>I conduct fundamental research in human-AI collaboration, advancing both new theoretical frameworks and novel collaboration paradigms. In parallel, I develop computational methods for user modeling—capturing preferences, capabilities, and behaviors—to design 
                            <b>human-aware AI</b> that supports adaptive human-AI interaction. These methods have been applied to enhance transparent robot learning from human demonstrations, calibrate appropriate human trust in collaborative decision-making, and improve the effectiveness of human-AI teamwork.</li>
                        <br>
                        <li>In the application part, I adopt human‑centered design to develop interactive AI systems to support users in solving real‑world problems across domains, including <b>Decision Making</b>, <b>Education & Learning</b>, 
                            <b>Work & Creation</b>, <b>Healthcare & Wellbeing</b>.</li>
                    </ul>
                    
                    <!-- My work is primarily published at the ACM CHI conference, where I have been honored with four Best Paper <span style="color: orange;">Honorable Mention Awards</span>. -->
                    <!--My research interest lies in Human-AI Interaction. Previously, I utilized advanced CV and NLP techniques to build interactive tools for users' daily activities (e.g., photo-taking, video-editing), Education (e.g., MOOC learning, online classes), and Healthcare (e.g., Parkinson’s disease detection, online health community support). Currently, I am focusing on fostering transparent communication in human-human interaction and human-AI interaction with human-centered approaches. Also, I am interested in human-centered explainable AI (HCXAI).-->

                </p>
				
                <!-- <p>
					I publish at ACM <a href="https://dl.acm.org/conference/chi">CHI</a>, <a href="https://dl.acm.org/conference/uist">UIST</a>, and <a href="https://dl.acm.org/journal/imwut"> IMWUT </a> and have received 2 Best Paper and 4 <span style="color: orange;">Honorable Mention Award</span>s. Taxonomies of my completed research can be found below.
                </p> -->
			 
            </div>
            <div class="offset-xl-0 col-lg-4 col-md-3">
                <img src="./personal_webpage_files/Me.jpg" class="portrait" alt="a portrait of shuai ma">
            </div>
        </div>
        
        




        <div>
            <img src="./personal_webpage_files/research_chart.png" class="portrait">
        </div>
        <br>
        <div class="row">
            <!-- left column -->
            <div class="col-lg-8 mb-2">
                <h2>
                    Selected Publications <font style="font-size: 15px;"></font>
					<!--
                    <div class="float-right">
                        <small class="ml-2">by category</small>
                        <div class="switch sort-by-date">
                            <span class="slider round"></span>
                        </div>
                    </div>
					-->
                </h2>
                
                <div class="filter-buttons">
                    <button onclick="filterPublications('Selected')">Selected</button>
                    <button onclick="filterPublications('All')">All</button>
                    <button onclick="filterPublications('Decision Making')">Decision Making</button>
                    <button onclick="filterPublications('Education')">Education</button>
                    <button onclick="filterPublications('Wellbeing')">Healthcare & Wellbeing</button>
                    <button onclick="filterPublications('Work')">Work & Creation</button>
                </div>

                <div class="research-projects">
					
                <!-- <div class="alert alert-secondary">
                    <h3>Please refer to <a href="https://www.hilab.dev/">HiLab's webpage</a> for more recent work (after 01/01/2021)</h3>
                </div> -->



                <div class="row research-project" data-sort="2023" data-category = "Decision Making,Human-AI Collaboration,All,Selected">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/deliberation.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>

                    <div class="col-md-8">
                        <h6>
							<i class="fa-solid fa-award" style="color: orange;"></i> Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making
                        </h6>
                        <p class="text-muted">
                            <b><u>Shuai Ma</u></b>, Qiaoyi Chen, Xinru Wang, Chengbo Zheng, Zhenhui Peng, Ming Yin, Xiaojuan Ma. (CHI 2025, <span style="color: orange;">Honorable Mention Award</span>)<br>
                            <a class="info", href="./personal_webpage_files/paper_pdf/Human-AI Deliberation.pdf">[PDF]</a>
                            
                        </p>
                       
                        <p>
							What happens when you disagree with your AI collaborator during decision-making?
                            We explore how to resolve such conflicts by leveraging the power of Retrieval-Augmented Generation. Acting as a communication bridge, a large language model enables deliberation between human decision-makers and domain-specific smaller models, significantly improving decision quality in the face of disagreement.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>



                <div class="row research-project" data-sort="2025" data-category = "All,Wellbeing">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/AI_afterlife.png">
                           <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
                           <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
                       
                    </div>
                    <div class="col-md-8">
                        <h6>
                            <i class="fa-solid fa-award" style="color: orange;"></i> "AI Afterlife" as Digital Legacy: Perceptions, Expectations, and Concerns
                        </h6>
                        <p class="text-muted">
                            Ying Lei, <b><u>Shuai Ma*</u></b>, Yuling Sun, Xiaojuan Ma. (CHI 2025, <span style="color: orange;">Honorable Mention Award</span>) (*: corresponding author)<br>

                            <a class="info", href="./personal_webpage_files/paper_pdf/AI_Afterlife.pdf">[PDF]</a>
                        </p>
                       
                        <p>
                            Can AI-generated digital humans serve as a form of digital legacy—allowing us to 'live on' in this world?
                            What are people’s perceptions, expectations, and concerns regarding this emerging possibility?
                            Our comprehensive qualitative analysis investigates the full lifecycle of the "AI afterlife," shedding light on how society envisions—and grapples with—this future.
                        </p>
                    </div>
                    
                    <div class="col-md-12"> 
                        <hr class="dash">
                    </div>
                </div>


                <div class="row research-project" data-sort="2025" data-category = "All,Selected,Education">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/DBox.png">
                           <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
                           <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
                       
                    </div>
                    <div class="col-md-8">
                        <h6>
                            DBox: Scaffolding Algorithmic Programming Learning through Learner-LLM Co-Decomposition
                        </h6>
                        <p class="text-muted">
                            <b><u>Shuai Ma</u></b>, Junling Wang, Yuanhao Zhang, Xiaojuan Ma, April Yi Wang. (CHI 2025)<br>

                            <a class="info", href="./personal_webpage_files/paper_pdf/DBox.pdf">[PDF]</a>
                        </p>
                       
                        <p>
                            Can LLMs be used to help learners learn algorithmic programming learning? We propose a Learn-LLM Co-Decomposition approach to make LLMs scaffold such learning process.
                        </p>
                    </div>
                    
                    <div class="col-md-12"> 
                        <hr class="dash">
                    </div>
                </div>


                <div class="row research-project" data-sort="2025" data-category = "All">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/Signal_human_intention.png">
                           <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
                           <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
                       
                    </div>
                    <div class="col-md-8">
                        <h6>
                            <i class="fa-solid fa-award" style="color: orange;"></i> Signaling Human Intentions to Service Robots: Understanding the Use of Social Cues during In-Person Conversations
                        </h6>
                        <p class="text-muted">
                            Hanfang Lyu, Xiaoyu Wang, Nandi Zhang, <b><u>Shuai Ma</u></b>, Qian Zhu, Yuhan Luo, Fugee Tsung, Xiaojuan Ma. (CHI 2025, <span style="color: orange;">Honorable Mention Award</span>)<br>

                            <a class="info", href="./personal_webpage_files/paper_pdf/Signaling Human Intentions.pdf">[PDF]</a>
                        </p>
                       
                        <p>
                            How do humans and service robots naturally express intentions through interactive actions in social settings? Our work offers valuable insights into natural human-robot interactions in the era of embodied intelligence, revealing how subtle, intuitive behaviors can serve as effective communicative cues.
                        </p>
                    </div>
                    
                    <div class="col-md-12"> 
                        <hr class="dash">
                    </div>
                </div>


                <div class="row research-project" data-sort="2025" data-category = "All,Wellbeing">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/scaffold_turns.png">
                           <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
                           <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
                       
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Scaffolded Turns and Logical Conversations: Designing Humanized LLM-Powered Conversational Agents for Hospital Admission Interviews

                        </h6>
                        <p class="text-muted">
                            Dingdong Liu, Yujing Zhang, Bolin Zhao, <b><u>Shuai Ma</u></b>, Chuhan Shi, Xiaojuan Ma. (CHI 2025)<br>

                            <a class="info", href="./personal_webpage_files/paper_pdf/Scaffolded Turns.pdf">[PDF]</a>
                        </p>
                       
                        <p>
                            We present a humanized conversational agent for hospital admissions that balances efficiency and empathy. Co-designed with clinicians, our system uses dynamic graph-based conversation flows and context-aware scaffolding to adaptively guide interviews. It outperforms existing solutions in both data accuracy and user experience, offering a promising path for AI-assisted healthcare interactions.
                        </p>
                    </div>
                    
                    <div class="col-md-12"> 
                        <hr class="dash">
                    </div>
                </div>


                <div class="row research-project" data-sort="2025" data-category = "All,Human-AI Collaboration">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/CCCF.png">
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							以双向理解促进人智协同——以人机合作决策为例
                        </h6>
                        <p class="text-muted">
                            <b><u>Shuai Ma</u></b>, Xiaojuan Ma, Chuhan Shi, Chengbo Zheng. (中国计算机学会通讯, Communications of the CCF, Volume 21， 2025)<br>
                            <a class="info", href="./personal_webpage_files/paper_pdf/CCCF.pdf">[PDF]</a>
                        </p>
                        <p>
							We promote human-AI hybrid intelligence from the perspective of mutual understanding between humans and AI. This paper systematically introduces our previous work on enabling human-AI hybrid intelligence by enhancing human understanding of AI, AI’s understanding of humans, and humans’ self-understanding.</p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>    



                <div class="row research-project" data-sort="2024" data-category = "Decision Making,Human-AI Collaboration,All">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/role.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							Beyond Recommender: An Exploratory Study of the Effects of Different AI Roles in AI-Assisted Decision Making
                        </h6>
                        <p class="text-muted">
                            <b><u>Shuai Ma</u></b>, Chenyi Zhang, Xinru Wang, Xiaojuan Ma, Ming Yin. (Working Paper, Arxiv 2024)<br>
                            <a class="info", href="./personal_webpage_files/paper_pdf/role.pdf">[PDF]</a>
                            
                        </p>
                       
                        <p>
							AI is often used as a Recommender in decision-making, but this can reduce human analytical thinking and cause over-reliance on AI. 
                            Unlike AI, human advisors play diverse roles in decisions. This paper examines three AI roles—Recommender, Analyzer, and Devil’s Advocate—at two performance levels. 
                            Results show each role's unique strengths and weaknesses in task performance, reliance, and user experience. 
                            Notably, the Analyzer role can be more effective than the Recommender when AI performance is low. These findings inform the design of adaptive AI assistants.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>












                <div class="row research-project" data-sort="2024" data-category = "Decision Making,Human-AI Collaboration,All,Selected">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/calibration.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							"Are You Really Sure?'' Understanding the Effects of Human Self-Confidence Calibration in AI-Assisted Decision Making
                        </h6>
                        <p class="text-muted">
                            <b><u>Shuai Ma</u></b>, Xinru Wang, Ying Lei, Chuhan Shi, Ming Yin, Xiaojuan Ma. (CHI 2024)<br>
                            <a class="info", href="./personal_webpage_files/paper_pdf/self_confidence_calibration.pdf">[PDF]</a>
                            
                        </p>
                       
                        <p>
							This paper investigates human self-confidence calibration in AI-assisted decision-making, conducting three user studies to analyze its effect on human-AI collaboration. The research examines the impact of self-confidence on AI reliance and tests three calibration mechanisms. Findings indicate that proper self-confidence calibration improves rational behavior and appropriate reliance in human-AI teams, offering insights into enhancing collaboration efficiency.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>

                
                <div class="row research-project" data-sort="2024" data-category = "Education,Human-AI Collaboration,All">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/AI4coursework.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							Charting the Future of AI in Project-Based Learning: A Co-Design Exploration with Students
                        </h6>
                        <p class="text-muted">
                            Chengbo Zheng, Kangyu Yuan, Bingcan Guo, Reza Hadi Mogavi, Zhenhui Peng, <b><u>Shuai Ma</u></b>, Xiaojuan Ma. (CHI 2024)<br>

                            <a class="info", href="./personal_webpage_files/paper_pdf/chartingFuture.pdf">[PDF]</a>
                        </p>
                       
                        <p>
							This study investigates using AI usage data for learning assessment in project-based education. Through workshops with college students, we explored how AI data can reflect students' skills and contributions. The findings highlight the potential and challenges of integrating AI data in educational assessments, informing the development of future educational tools for data analysis and presentation.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>

                <div class="row research-project" data-sort="2024" data-category = "Wellbeing,All">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/ICT.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							Unpacking ICT-supported Social Connections and Support of Late-life Migration: From the Lens of Social Convoys
                        </h6>
                        <p class="text-muted">
                            Ying Lei, <b><u>Shuai Ma</u></b>, Yuling Sun. (CHI 2024)<br>

                            <a class="info", href="./personal_webpage_files/paper_pdf/late_life_migration.pdf">[PDF]</a>
                        </p>
                       
                        <p>
							This paper explores the ICT-mediated social connections of late-life migrants, focusing on the dynamic changes in their social networks and the roles of ICT in these shifts. Utilizing the social convoy model, we examine the evolving support roles within migrants' networks, alongside the challenges and expectations related to ICT-supported social connections. Our findings offer in-depth insights into the social connections and support systems of late-life migrants, culminating in design implications for future ICT-based support systems for this demographic.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>

                <div class="row research-project" data-sort="2024" data-category = "Work,Human-AI Collaboration,All">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/humanAIFE.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							Towards Feature Engineering with Human and AI's Knowledge: Understanding Data Workers' Perceptions in Human&AI-Assisted Feature Engineering Design
                        </h6>
                        <p class="text-muted">
                            Qian Zhu, Dakuo Wang, <b><u>Shuai Ma</u></b>, April Wang, Zixin Chen, Udayan Khurana, Xiaojuan Ma. (DIS 2024)<br>
                            <!-- <a class="info">[To Appear]</a> -->
                            <a class="info", href="./personal_webpage_files/paper_pdf/humanAI_FE.pdf">[PDF]</a>
                        </p>
                       
                        <p>
							How do data scientists view suggestions from other peers and AI when performing feature engineering? When the two are presented to data scientists at the same time, how will they choose? What are the key deficiencies in today’s AI when recommending features? In this article, we designed a human-AI collaborative feature engineering framework and invited 14 data scientists to conduct experiments.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>
                

                <div class="row research-project" data-sort="2024" data-category = "All,Human-AI Collaboration">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/XAIsurvey.png">
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							Towards Human-centered Design of Explainable Artificial Intelligence (XAI): A Survey of Empirical Studies
                        </h6>
                        <p class="text-muted">
                            <b><u>Shuai Ma</u></b> (Working paper, Arxiv 2024)<br>
                            <a class="info", href="./personal_webpage_files/paper_pdf/XAIsurvey.pdf">[PDF]</a>
                        </p>
                        <p>
							We conduct a survey study on XAI from empirical study perspectives, highlighting the importance of designing Human-centered XAI methods from an explainee's (whom being explained to, e.g., end users or stakeholders) perspective.</p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>    



                <div class="row research-project" data-sort="2024" data-category = "All,Human-AI Collaboration">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/self-gause.png">
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							SelfGauge: An Intelligent Tool to Support Student Self-assessment in GenAI-enhanced Project-based Learning
                        </h6>
                        <p class="text-muted">
                            Chengbo Zheng, Zeyu Huang, <b><u>Shuai Ma</u></b>, Xiaojuan Ma (UIST 2024 Adjunct)<br>
                            <a class="info", href="./personal_webpage_files/paper_pdf/self-gauge.pdf">[PDF]</a>
                        </p>
                        <p>
							Project-based learning (PBL) involves students tackling real-world problems and creating artifacts. With the rise of generative AI (GenAI) tools, assessing students in GenAI-enhanced PBL is challenging. To address this, we designed SelfGauge, a tool that supports student self-assessment by analyzing their GenAI usage and project artifacts. It helps students define criteria, seek feedback, and reflect on their performance, promoting continuous self-improvement.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>    



                <div class="row research-project" data-sort="2023" data-category = "Decision Making,Human-AI Collaboration,All,Selected">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/human_ai_CL.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							Who Should I Trust: AI or Myself? Leveraging Human and AI Correctness Likelihood to Promote Appropriate Trust in AI-Assisted Decision-Making
                        </h6>
                        <p class="text-muted">
                            <b><u>Shuai Ma</u></b>, Ying Lei, Xinru Wang, Chengbo Zheng, Chuhan Shi, Ming Yin, Xiaojuan Ma. (CHI 2023)<br>

                            <a class="info", href="./personal_webpage_files/paper_pdf/human_AI_CL.pdf">[PDF]</a>
                            <a class="info", href="https://github.com/mashuaiwudi/mashuaiwudi.github.io/tree/master/HAIcollaboration">[Code]</a>
                            <a class="info", href="https://userstudy.link/HAIcollaboration/index.html">[Live Demo]</a>
                            <a class="info", href="https://www.youtube.com/watch?v=AWdUDaEqoSs">[Video]</a>
                        </p>
                       
                        <p>
							We proposed to promote humans' appropriate trust based on the correctness likelihood of both sides at a task-instance level. Results from a between-subjects experiment (N=293) showed that our CL exploitation strategies promoted more appropriate human trust in AI, compared with only using AI confidence.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>
                
                <div class="row research-project" data-sort="2023" data-category = "Decision Making,Work,Human-AI Collaboration,All">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/retrolens.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							RetroLens: A Human-AI Collaborative System for Multi-step Retrosynthetic Route Planning
                        </h6>
                        <p class="text-muted">
                            Chuhan Shi, Yicheng Hu, Shenan Wang, <b><u>Shuai Ma</u></b>, Chengbo Zheng, Xiaojuan Ma, Qiong Luo. (CHI 2023)<br>

							<!-- <a class="info" href="https://vimeo.com/394833754">[Video]</a> -->
                            <!-- <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3432232">[DOI]</a> -->
                            <a class="info", href="./personal_webpage_files/paper_pdf/RetroLens.pdf">[PDF]</a>
                        </p>
                       
                        <p>
							Targeting Multi-step Human-AI Collaboration task for chemists, we proposed a human-AI collaborative system, RetroLens, through a participatory design process. AI can contribute by two approaches: joint action and algorithm-inthe-loop.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>

                <div class="row research-project" data-sort="2023" data-category = "Decision Making,Education,Human-AI Collaboration,All">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/aeser.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							Competent but Rigid: Identifying the Gap in Empowering AI to Participate Equally in Group Decision-Making
                        </h6>
                        <p class="text-muted">
                            Chengbo Zheng, Yuheng Wu, Chuhan Shi, <b><u>Shuai Ma</u></b>, Jiehui Luo, Xiaojuan Ma. (CHI 2023)<br>

							<!-- <a class="info" href="https://vimeo.com/394833754">[Video]</a> -->
                            <!-- <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3432232">[DOI]</a> -->
                            <a class="info", href="./personal_webpage_files/paper_pdf/Competent but Rigid.pdf">[PDF]</a>
                        </p>
                       
                        <p>
							What will happen if AI participate equally in human group decision-making? We studied this problem in teacher-AI collaborative decision-making. We find that although the voice of AI is considered valuable, AI still plays a secondary role in the group because it cannot fully follow the dynamics of the discussion and make progressive contributions. Moreover, the divergent opinions
                            of our participants regarding an "equal AI" shed light on the possible future of human-AI relations.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>

                


                <div class="row research-project" data-sort="2022" data-category = "Education,Human-AI Collaboration,All,Selected">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/learning_engagement.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							Modeling Adaptive Expression of Robot Learning Engagement and Exploring its Effects on Human Teachers
                        </h6>
                        <p class="text-muted">
                            <b><u>Shuai Ma</u></b>, Mingfei Sun, Xiaojuan Ma. (TOCHI 2022)<br>
                            <a class="info", href="./personal_webpage_files/paper_pdf/LearningEngagement.pdf">[PDF]</a>
                            <a class="info", href="https://github.com/mashuaiwudi/mashuaiwudi.github.io">[Code]</a>
                            <a class="info", href="https://userstudy.link/">[Live Demo]</a>
                            <a class="info", href="https://www.youtube.com/watch?v=qm5YQl3Mj7Q">[Video]</a>
                        </p>
                       
                        <p>
							For human-robot teaching scenario, we propose an adaptive modeling and expression method to facilitate the transparent communication of robots' learning statuses during human demonstration.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>


					
                <div class="row research-project" data-sort="2022" data-category = "Education,Human-AI Collaboration,All,Selected">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/glancee.png">
						   <!-- <source type="video/mp4" src="/research/MorphingCircuit/thumbnail.mp4">
						   <source type="video/webm" src="/research/MorphingCircuit/thumbnail.webm"> -->
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							Glancee: An Adaptable System for Instructors to Grasp Student Learning Status in Synchronous Online Classes
                        </h6>
                        <p class="text-muted">
                            <b><u>Shuai Ma</u></b>, Taichang Zhou, Fei Nie, Xiaojuan Ma. (CHI 2022)<br>

							<!-- <a class="info" href="https://vimeo.com/394833754">[Video]</a> -->
                            <!-- <a class="info" href="https://dl.acm.org/doi/abs/10.1145/3432232">[DOI]</a> -->
                            <a class="info", href="./personal_webpage_files/paper_pdf/glancee.pdf">[PDF]</a>
                            <a class="info", href="https://github.com/MiracleShuai/MiracleShuai.github.io">[Code]</a>
                            <a class="info", href="https://miracleshuai.github.io/index.html">[Live Demo]</a>
                            <a class="info", href="https://www.youtube.com/watch?v=QPMF5qOK6yI">[Video]</a>
                        </p>
                       
                        <p>
							Focusing on synchronous online classes (e.g., real-time Zoom-based classes), Glancee address instructors’ difficulty to observe students’ learning status due to students’ unwillingness to show their videos. Specifically, we mitigate the gap that lack of empirical investigation on instructors’ preferences and lack of exploration of designing adaptable systems to meet the needs of individual instructors. 
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>

                <div class="row research-project" data-sort="2022" data-category = "Wellbeing,Human-AI Collaboration,All">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/cass.png">
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							CASS: Towards Building A Social-Support Chatbot for Online Health Community
                        </h6>
                        <p class="text-muted">
                            Liuping Wang, Dakuo Wang, Feng Tian, Zhenhui Peng, Xiangmin Fan, Zhan Zhang, <b><u>Shuai Ma</u></b>, Mo Yu, Xiaojuan Ma, Hongan Wang. (CSCW 2021)<br>

                            <a class="info", href="./personal_webpage_files/paper_pdf/cass.pdf">[PDF]</a>
                        </p>
                       
                        <p>
							We investigated how chatbots can be designed to provide information and emotional support for pregnant women in an online health community. 
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>


                <div class="row research-project" data-sort="2019" data-category = "Work,Human-AI Collaboration,All,Selected">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/smarteye.png">
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							<i class="fa-solid fa-award" style="color: orange;"></i> SmartEye: Assisting Instant Photo Taking via Integrating User Preference with Deep View Proposal Network
                        </h6>
                        <p class="text-muted">
                            <b><u>Shuai Ma</u></b>, Zijun Wei, Feng Tian, Xiangmin Fan, Jianming Zhang, Xiaohui Shen, Zhe Lin, Jin Huang, Radomir Mech, Dimitris Samaras, Hongan Wang. (CHI 2019, <span style="color: orange;">Honorable Mention Award</span>)<br>
                            <a class="info", href="./personal_webpage_files/paper_pdf/SmartEye.pdf">[PDF]</a>
                        </p>
                       
                        <p>
							How to effectively personalize a general model? 
                            We proposed a user preference modeling method based on interactive machine learning and designed a confidence-based integration framework to personalize a deep neural network to cater to users' individual preferences in photo composition. 
                            Based on the proposed algorithm, we designed SmartEye, which can gradually learn users' preferences as the interaction goes on.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>


                <div class="row research-project" data-sort="2019" data-category = "Work,Human-AI Collaboration,All">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/prescreen.png">
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							Pre-screen: Assisting Material Screening in Early-stage of Video Editing
                        </h6>
                        <p class="text-muted">
                            Qian Zhu*, <b><u>Shuai Ma*</u></b>, Cuixia Ma (UIST 2019 Adjunct, *: equal contribution)<br>
                            <a class="info", href="./personal_webpage_files/paper_pdf/prescreen.pdf">[PDF]</a>
                        </p>
                        <p>
							Video editing is challenging for both professionals and amateurs due to the time-consuming task of screening useful clips from raw footage. 
                            To address these difficulties, we conducted a pilot study involving surveys and interviews with 20 participants. 
                            Based on our findings, we developed Pre-screen, an innovative tool that offers global and detailed video analysis, as well as intelligent material screening features using advanced video processing and visualization methods.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>  


                <div class="row research-project" data-sort="2019" data-category = "Education,All">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/reminder.png">
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							What Did I Miss? Assisting User-adaptive Missed Content Reviewing in MOOC Learning
                        </h6>
                        <p class="text-muted">
                            Qian Zhu*, <b><u>Shuai Ma*</u></b> (UIST 2019 Adjunct, *: equal contribution)<br>
                            <a class="info", href="./personal_webpage_files/paper_pdf/reminder.pdf">[PDF]</a>
                        </p>
                        <p>
							In Massive Open Online Courses (MOOCs), learners often face distractions leading to divided attention (DA). This paper introduces Reminder, a system that detects DA using cameras on PC and mobile devices.
                             It predicts attention scores with a regression model and adapts to individual users. Reminder also offers visualizations to help learners easily review missed content. 
                             User studies demonstrate its effectiveness in detecting and assisting learners with missed course content.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>  



                <div class="row research-project" data-sort="2019" data-category = "Wellbeing,All">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/workshop.png">
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							Human-AI Interaction in Healthcare: Three Case Studies About How Patient(s) And Doctors Interact with AI in a Multi-Tiers Healthcare Network
                        </h6>
                        <p class="text-muted">
                            Yunzhi Li, Liuping Wang, <b><u>Shuai Ma</u></b>, Xiangmin Fan, Zijun Wang, Junfeng Jiao, Dakuo Wang (CHI 2019 Workshop Paper)<br>
                            <a class="info", href="./personal_webpage_files/paper_pdf/HealthWorkshopCHI19.pdf">[PDF]</a>
                        </p>
                        <p>
							This position paper introduces three ongoing research projects focused on designing, developing, and evaluating systems for human-AI interaction in healthcare. 
                            Collaborating with local government administrators, hospitals, clinics, and doctors in a Beijing suburb, we study how AI technologies are transforming healthcare delivery and reception. 
                            We aim to foster discussion at the workshop and establish collaborations within the health informatics community.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div> 

                <div class="row research-project" data-sort="2019" data-category = "Wellbeing,All">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/smartphonepd.png">
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							Implicit detection of motor impairment in Parkinson's disease from everyday smartphone interactions
                        </h6>
                        <p class="text-muted">
                            Jing Gao, Feng Tian, Junjun Fan, Dakuo Wang, Xiangmin Fan, Yicheng Zhu, <b><u>Shuai Ma</u></b>, Jin Huang, Hongan Wang. (CHI 2018 LBW)<br>
                            <a class="info", href="./personal_webpage_files/paper_pdf/smartphonepd.pdf">[PDF]</a>
                        </p>
                        <p>
							In this work, we explored the feasibility and accuracy of detecting motor impairment in Parkinson’s disease (PD) via implicitly sensing and analyzing users’ everyday interactions with their smartphones.
                            Through a 42 subjects study, our approach achieved an overall accuracy of 88.1% (90.0%/86.4% sensitivity/specificity) in discriminating PD subjects from age-matched healthy controls.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>


                

                <div class="row research-project" data-sort="2019" data-category = "Wellbeing,All">
                    <div class="col-md-4">
                       <video loop="" muted="" playsinline="" poster="./personal_webpage_files/paper_teaser/mirroru.png">
                       </video>
					   
                    </div>
                    <div class="col-md-8">
                        <h6>
							mirrorU: scaffolding emotional reflection via in-situ assessment and interactive feedback
                        </h6>
                        <p class="text-muted">
                            Liuping Wang, Xiangmin Fan, Feng Tian, Lingjia Deng, <b><u>Shuai Ma</u></b>, Jin Huang, Hongan Wang (CHI 2018 LBW)<br>
                            <a class="info", href="./personal_webpage_files/paper_pdf/mirroru.pdf">[PDF]</a>
                        </p>
                        <p>
							We present mirrorU, a mobile system that supports users to reflect on and write about their daily emotional experience. 
                            While prior work has focused primarily on providing memory triggers or affective cues, mirrorU provides in-situ assessment and interactive feedback to scaffold reflective writing. 
                            It automatically and continuously monitors the composition process in three dimensions (i.e., level of detail, overall valence, and cognitive engagement) and provides relevant feedback to support reflection.
                        </p>
                    </div>
					
					<div class="col-md-12"> 
						<hr class="dash">
					</div>
                </div>                



            </div>
            </div>
            <!-- /left column -->
            <!-- right column -->
            <div class="col-lg-4 mb-2">
				


                
                <!-- <h2>Upcoming Travel</h2>
                <ul class="news" style="font-size: 13px">
                    <li><b>Oct 2025</b> | CSCW 2025, Bergen, Norway</li>
                    <li><b>Oct 2025</b> | Ubicomp 2025, Espoo, Finland</li>
                </ul> -->

                <h2>Latest News</h2>
                <ul class="news" style="font-size: 13px">
                    <!-- <li><span class="strong">Sep 2024:</span> Arrived in Helsink.</li> -->
                    <li><span class="strong">March 2025:</span> Three papers got ACM CHI 2025 <span style="color: orange;">Honorable Mention Award</span> <i class="fa-solid fa-award" style="color: orange;"></i>! Thanks to my collaborators!</li>
                    <li><span class="strong">Jan 2025:</span> Five papers got accpted by ACM CHI 2025! Thanks to all collaborators!</li>
                    <li><span class="strong">Aug 2024:</span> Become Dr. Ma!</li>
                    <li><span class="strong">May 2024:</span> Passed my PhD thesis proposal defense.</li>
                    <li><span class="strong">May 2024:</span> Attending CHI 2024 @Hawaii</li>
                    <li><span class="strong">Apr 2024:</span> Our paper about Human-AI Collaborative Feature Engineering got accepted by ACM DIS 2024! Congrats to my co-authors.</li>
                    <li><span class="strong">Feb 2024:</span> Start my PC work for FAccT 2024.</li>
                    <li><span class="strong">Feb 2024:</span> Start my AC work for CHI 2024 LBW.</li>
                    <li><span class="strong">Jan 2024:</span> Three papers got accepted by CHI 2024! Congrats to my co-authors.</li>
                    <li><span class="strong">Nov 2023:</span> Arrived in ETH Zurich. Visiting Prof. Wang's Peach Lab.</li>
                    <li><span class="strong">Apr 2023:</span> Reconnect at CHI 2023! @Hamburg.</li>
                    <li><span class="strong">Jan 2023:</span> Three papers got accepted by CHI 2023! Congrats to my co-authors.</li>

                  
                </ul>
                <ul class="news" id="news-more" style="font-size: 13px">
                    

                    <li><span class="strong">Aug 2022:</span> Our paper 'Modeling Adaptive Expression of Robot Learning Engagement' has been accepted by TOCHI!</li>
                    <li><span class="strong">Apr 2022:</span> I passed my PhD Qualifying Exam and became a PhD candidate! Thanks for my committee members' valuable feedback!</li>
                    <li><span class="strong">Mar 2024:</span> Happy to be a student volunteer at CHI 2022!</li>
                    <li><span class="strong">Feb 2022:</span> Our paper <i>Glancee</i> is conditionally accepted at CHI 2022.</li>
                </ul>
                <a id="toggle-more-news" href="#">More &gt;</a>
				<br/>
				<br/>
				

                <h2>Service</h2>
                <b>Program Committee</b>
                <ul class="news" style="font-size: 13px">
                    <li><b>2026</b> | ACM CHI 2026 (understanding people)</li>
                    <li><b>2025</b> | ACM CHI 2025 (computational interaction)</li>
                    <li><b>2025</b> | ACM DIS 2025</li>
                    <li><b>2024</b> | ACM FAccT 2024</li>
                    <li><b>2024</b> | ACM CHI LBW 2024</li>
                    <li><b>2023</b> | ACM CHI LBW 2023</li>
                </ul>
                <b>Reviewer</b>
                <ul class="news" style="font-size: 13px">
                    <li><b>ACM CHI</b> | 2026, 2025<sup> (1)</sup>, 2024<sup> (2)</sup>, 2023<sup> (3)</sup>, 2022, 2021, 2020, 2019</li>
                    <li><b>ACM CHI LBW</b> | 2024, 2023, 2022</li>
                    <li><b>ACM TOCHI</b> | 2025, 2024, 2023, 2022</li>
                    <li><b>ACM UIST</b> | 2025, 2024, 2023</li>
                    <li><b>IJHCS</b> | 2024, 2023</li>
                    <li><b>ACM DIS</b> | 2025<sup> (4)</sup></li>
                    <li><b>IEEE VIS</b> | 2025</li>
                    <li><b>Journal: Human-Computer Interaction</b> | 2024</li>
                    <li><b>ACM FAccT</b> | 2024</li>
                    <li><b>ACM TiiS</b> | 2024, 2023</li>
                    <li><b>ACM CSCW</b> | 2025, 2023</li>                    
                    <li><b>ACM WWW</b> | 2021</li>
                    
                    (1-4)<i class="fa-solid fa-award" style="color: orange;"></i>received Special Recognition for Outstanding Reviews for CHI 2023, CHI 2024, CHI 2025, DIS 2025
                </ul>
                <b>Other Service</b>
                <ul class="news" style="font-size: 13px">
                    <li><b>Student Volunteer</b> | ACM CHI 2023<sup> (4)</sup>, CHI 2022<sup> (5)</sup>, CHI 2019 China Night</li>
                    <li><b>Web Master</b> | CHI 2019 Workshop</li>
                    (4-5)<i class="fa-solid fa-award" style="color: orange;"></i>received Student Volunteer Award for CHI 2022, CHI 2023
                </ul>


                <br>
                <h2>Teaching</h2>
                <ul class="news" style="font-size: 13px">
                    <li>2024-2025 Spring ELEC‑E7871 - Advanced Topics in Human‑Computer Interaction (Guest Lecturer, @Aalto University)</li>
                    <li>2023-2024 Spring COMP4321 - Search Engines for Web (TA, @HKUST)</li>
					<li>2021-2022 Fall COMP 1021 - Introduction to Computer Science (TA, @HKUST)</li>
					<li>2020-2021 Spring COMP 1021 - Introduction to Computer Science (TA, @HKUST)</li>
                    <li>2019 August - CCF courses (Teaching Volunteer, @Lvliang, Shanxi, China)</li>
                    <li>2018-2019 Introduction to Natural User Interfaces (Lecturer, @Beijing Zhongguancun No.1 Primary School)</li>
                    <li>2018 Introduction to Natural User Interfaces (Lecturer, @Beijing Science and Innovation Open Day)</li>
                </ul>
				<br>


                <h2>Awards</h2>
                <ul class="award" style="font-size: 13px">
                    <li>2025 Honorable Mention Award @CHI 2025 * 3</li>
                    <li>2024 Best Paper Award #SIGCHI GBA Chapter Pre-CHI 2024</li>
                    <li>2024 HKUST RedBird Academic Excellence Award</li>
                    <li>2023 HKUST PhD Overseas Research Award</li>
                    <li>2023 SIGCHI Gary MarsdenTravel Awards</li>
                    <li>2020 HKUST Redbird PhD Scholarship</li>
					<li>2019 Honorable Mention Award @CHI 2019</li>
					<li>2019 President Scholarship in Chinese Academy of Sciences (1% selected)</li>
					<li>2018 National Scholarship for Graduate (1% selected)</li>
					<li>2017 Excellence Award for Science Creation Program of Chinese Academy of Sciences</li>
					<li>2017 Special Scholarship for Undergraduates (0.1% selected)</li>
                    <li>2016 National Scholarship for Undergraduate (1% selected)</li>
                </ul>
				<br>

                <h2>Academia Experience</h2>
                <ul class="award" style="font-size: 13px">
                    <li><b>ETH Zurich</b>, with Prof. April Yi Wang, Visiting Scholar, 2023.10-2024.1</li>
                    <li><b>Purdue University</b>, with Prof. Ming Yin, Visiting Scholar, 2023.2-2023.7 (remote)</li>
                </ul>
				<br>
        </div>

    </div>

<footer>
    <div class="container">
		<small> © 2022 - 2025 All rights reserved. Webpage template from <a href="https://www.yangzhang.dev/">Yang Zhang</a></small>
        <!-- <div style="margin-top:20px;" class="col-lg-8 mb-8"><p class="annotation">[Research focus diagram inspired by professor <a href="https://people.eecs.berkeley.edu/~bjoern/">Bjoern Hartmann</a>]</p></div> -->
	</div>
        
</footer>





<script src="https://cdn.jsdelivr.net/npm/jquery@3.4/dist/jquery.min.js"></script>
<script src="https://kit.fontawesome.com/1b68509e26.js" crossorigin="anonymous"></script>
<script>

    $('#toggle-more-award').click(function () {
        $('#award-more').slideToggle(function() {
            // 在动画完成后检查可见性
            if ($('#award-more').is(':visible')) {
                $('#toggle-more-award').text('< Hide');
            } else {
                $('#toggle-more-award').text('More >');
            }
        });
        return false;
    });

    $('#toggle-more-news').click(function () {
        $('#news-more').slideToggle(function() {
            // 在动画完成后检查可见性
            if ($('#news-more').is(':visible')) {
                $('#toggle-more-news').text('< Hide');
            } else {
                $('#toggle-more-news').text('More >');
            }
        });
        return false;
    });



    document.querySelectorAll('.email-anchor').forEach(function(a) {
        a.href = 'mailto:' + ['shuai.ma', 'connect.ust.hk'].join('@');
    });



    function filterPublications(category) {
    // 获取所有出版物
    var publications = document.getElementsByClassName('research-project');

    // 循环遍历所有出版物
    for (var i = 0; i < publications.length; i++) {
        var publicationCategories = publications[i].getAttribute('data-category').split(',');

        if (category == 'all' || publicationCategories.includes(category)) {
            publications[i].style.display = ''; // 显示
        } else {
            publications[i].style.display = 'none'; // 隐藏
        }
    }
}

filterPublications('Selected');

</script>
